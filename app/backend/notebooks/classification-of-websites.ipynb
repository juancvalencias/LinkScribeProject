{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import lxml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import spacy as sp\n",
    "#import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_url</th>\n",
       "      <th>cleaned_website_text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.booking.com/index.html?aid=1743217</td>\n",
       "      <td>official site good hotel accommodation big sav...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://travelsites.com/expedia/</td>\n",
       "      <td>expedia hotel book sites like use vacation wor...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://travelsites.com/tripadvisor/</td>\n",
       "      <td>tripadvisor hotel book sites like previously d...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.momondo.in/?ispredir=true</td>\n",
       "      <td>cheap flights search compare flights momondo f...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...</td>\n",
       "      <td>bot create free account create free account si...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         website_url  \\\n",
       "0     https://www.booking.com/index.html?aid=1743217   \n",
       "1                   https://travelsites.com/expedia/   \n",
       "2               https://travelsites.com/tripadvisor/   \n",
       "3              https://www.momondo.in/?ispredir=true   \n",
       "4  https://www.ebookers.com/?AFFCID=EBOOKERS-UK.n...   \n",
       "\n",
       "                                cleaned_website_text Category  \n",
       "0  official site good hotel accommodation big sav...   Travel  \n",
       "1  expedia hotel book sites like use vacation wor...   Travel  \n",
       "2  tripadvisor hotel book sites like previously d...   Travel  \n",
       "3  cheap flights search compare flights momondo f...   Travel  \n",
       "4  bot create free account create free account si...   Travel  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"website_classification.csv\")\n",
    "df = dataset[['website_url','cleaned_website_text','Category']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Travel'],\n",
       "       ['Social Networking and Messaging'],\n",
       "       ['News'],\n",
       "       ['Streaming Services'],\n",
       "       ['Sports'],\n",
       "       ['Photography'],\n",
       "       ['Law and Government'],\n",
       "       ['Health and Fitness'],\n",
       "       ['Games'],\n",
       "       ['E-Commerce'],\n",
       "       ['Forums'],\n",
       "       ['Food'],\n",
       "       ['Education'],\n",
       "       ['Computers and Technology'],\n",
       "       ['Business/Corporate'],\n",
       "       ['Adult']], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.Category.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'category_id' with encoded categories \n",
    "df['category_id'] = df['Category'].factorize()[0]\n",
    "category_id_df = df[['Category', 'category_id']].drop_duplicates()\n",
    "\n",
    "\n",
    "# Dictionaries for future use\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Category']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "# We transform each cleaned_text into a vector\n",
    "features = tfidf.fit_transform(df.cleaned_website_text).toarray()\n",
    "\n",
    "labels = df.category_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_website_text'] # Collection of text\n",
    "y = df['Category'] # Target or the labels we want to predict\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camil\\OneDrive\\Desktop\\LinkScribeProject\\models\\.venv\\Lib\\site-packages\\sklearn\\calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df['category_id'], \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "m = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)\n",
    "m1=CalibratedClassifierCV(estimator=m,\n",
    "                                        cv=\"prefit\").fit(tfidf_vectorizer_vectors, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf_vectorizer.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the LinearSVC model\n",
    "joblib.dump(m, \"web_classifier.joblib\")\n",
    "\n",
    "# Save the CalibratedClassifierCV model\n",
    "joblib.dump(m1, \"cal_web_classifier.joblib\")\n",
    "\n",
    "# Save the Tf vectorizer\n",
    "joblib.dump(fitted_vectorizer, \"tf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education\n"
     ]
    }
   ],
   "source": [
    "from scraptool import ScrapTool\n",
    "website='https://www.mit.edu/'\n",
    "scraper = ScrapTool()\n",
    "\n",
    "\n",
    "try:\n",
    "    web=dict(scraper.visit_url(website))\n",
    "    text = web.get('website_text','')\n",
    "    t=fitted_vectorizer.transform([text])\n",
    "    print(id_to_category[m1.predict(t)[0]])\n",
    "    \n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MIT - Massachusetts Institute of TechnologyMassachusetts Institute of Technology Spotlight: Mar 28, 2025 Mar 28, 2025 Share:Skip to content ↓ Education Research Innovation Admissions + Aid Campus Life News Alumni About MIT More ↓ Admissions + Aid Campus Life News Alumni About MIT Menu ↓ Search Menu Explore websites, people, and locations Look up people by “last name sounds like” What are you looking for? See More Results Suggestions or feedback? MIT's response to government activity : Read the latest about government activity affecting MIT. Updates from campus : Read the latest from MIT and its leaders regarding events on campus. Spotlight: Mar 28, 2025 Students in the new class 16.811 (Advanced Manufacturing for Aerospace Engineers) design, build, and test an electric rocket engine turbopump. Zachary Cordero says they “don’t just learn how to solve a problem set — they learn how to be an engineer.” Full story Twitter Facebook MORE FROM THE MIT COMMUNITY IN THE MEDIA Cathy Fang spoke with CBS News about her research studying the effects of AI chatbots on people’s emotional well-being. “Overall, we found that extended use is correlated with more negative outcomes,” she said. RESEARCH Technology developed by mechanical engineers makes pesticides stick to plant leaves. With the new system, farmers could significantly cut their use of pesticides and fertilizers, saving money and reducing runoff. COMMUNITY SPOTLIGHT “I study household financial decision-making,” says MIT Sloan’s Christopher Palmer. “Both how households make decisions and how those decisions are influenced by external factors. That covers a lot of things.” AROUND CAMPUS The women's swimming and diving team won the NCAA Division III National Championship, its first national championship. MIT entered ranked as the top team in the country and came away with three individual national titles and four relay titles. AROUND CAMPUS Campus came alive with artistic energy earlier this month as Artfinity — the Institute's new festival celebrating creativity and community — took over multiple venues with interactive experiences, exhibitions, and performances. RESEARCH Engineers have developed a way to deliver long-lasting drugs with less pain: By injecting them as a suspension of tiny crystals, “we can have very controlled, sustained delivery, likely for multiple months and even years,” says Giovanni Traverso. Massachusetts Institute of Technology Education Research Innovation Admissions + Aid Campus Life News news.mit.edu About MIT Alumni Join us in building a better world. Massachusetts Institute of Technology 77 Massachusetts Avenue, Cambridge, MA, USA Visit Map Events People Careers Contact Privacy Accessibility Social Media Hub MIT@twitter MIT@facebook MIT@youtube MIT@instagram\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscraptool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CleanText\n\u001b[32m      2\u001b[39m cleaner = CleanText()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m clean_text = \u001b[43mcleaner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_cleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m clean_text = cleaner.basic_cleaning(clean_text)\n\u001b[32m      5\u001b[39m token_text = cleaner.tokenize_text(clean_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\camil\\OneDrive\\Desktop\\LinkScribeProject\\models\\scraptool.py:62\u001b[39m, in \u001b[36mCleanText.text_cleaning\u001b[39m\u001b[34m(self, input_text)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtext_cleaning\u001b[39m(\u001b[38;5;28mself\u001b[39m,input_text):\n\u001b[32m     61\u001b[39m     model_name = \u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m.from_pretrained(model_name)\n\u001b[32m     63\u001b[39m     soup = BeautifulSoup(input_text, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m     cleaned_text = soup.get_text()\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from scraptool import CleanText\n",
    "cleaner = CleanText()\n",
    "clean_text = cleaner.text_cleaning(text)\n",
    "clean_text = cleaner.basic_cleaning(clean_text)\n",
    "token_text = cleaner.tokenize_text(clean_text)\n",
    "text_to_generate = cleaner.tokens_to_text(token_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "\n",
    "def generate_text(prompt,max_length = 150):\n",
    "    inputs_ids = tokenizer.encode(prompt, return_tensors ='pt')\n",
    "    output = model.generate(inputs_ids, max_length = max_length)\n",
    "    generate_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
    "    return generate_text\n",
    "\n",
    "prompt = \"Summarize this text with key words\"\n",
    "generate_text = generate_text(prompt)\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Assume you have a pre-trained T5 tokenizer and model loaded\n",
    "model_name = \"gpt2\"  # Or any other T5 variant\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Your list of tokens (as strings)\n",
    "token_list = token_text\n",
    "\n",
    "# 1. Convert the list of string tokens to token IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(token_list)\n",
    "\n",
    "# 2. Convert the list of token IDs to a PyTorch tensor\n",
    "input_tensor = torch.tensor([input_ids])  # Add a batch dimension\n",
    "\n",
    "# 3. Generate text using the model\n",
    "# You might need to adjust the generation parameters based on your needs\n",
    "output_ids = model.generate(\n",
    "    input_tensor,\n",
    "    max_length=100,  # Maximum length of the generated text\n",
    "    num_return_sequences=1,  # Generate a single sequence\n",
    "    temperature=1.0,  # Controls the randomness of the output\n",
    "    # Add other generation parameters as needed (e.g., top_k, top_p)\n",
    ")\n",
    "\n",
    "# 4. Decode the generated token IDs back to a text string\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#print(\"List of Tokens:\", token_list)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Similitudes entre \n",
    "\n",
    "news - business and corporate\n",
    "\n",
    "Dificultades para clasificar\n",
    "\n",
    "e-commerce\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
